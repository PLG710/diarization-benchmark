{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b951ed03",
   "metadata": {},
   "source": [
    "Pipeline A: Whisper -> LLM-Diarisierung\n",
    "Kein Chunking, kein JSON\n",
    "Output: [Sprecher 1]: ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports + Konfiguration\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "VLLM_BASE_URL = os.getenv(\"VLLM_BASE_URL\", \"http://127.0.0.1:8005/v1\")\n",
    "VLLM_MODEL = os.getenv(\"VLLM_MODEL\", \"openai/gpt-oss-120b\")\n",
    "\n",
    "# Repository-Root: individuell anpassen\n",
    "REPO_ROOT = Path(\"~/jupyter/diarization-benchmark\").expanduser().resolve()\n",
    "\n",
    "# Pfad zu gespeicherten Audios\n",
    "AUDIO_DIR = REPO_ROOT / \"data\" / \"input_audio\"\n",
    "\n",
    "# Test-Audio-Datei (Benennen mit test1.mp3, ggf. anpassen)\n",
    "TEST_AUDIO = REPO_ROOT /\"data\" / \"test_audio\" / \"test2.mp3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Audio-Normalisierung via ffmpeg, alles konvertieren zu WAV 16khz Mono -> Wichtig für Pyannote\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_wav_16k_mono(input_path: str | Path, out_dir: str | Path) -> Path:\n",
    "    input_path = Path(input_path)\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_path = out_dir / f\"{input_path.stem}_16k_mono.wav\"\n",
    "\n",
    "    # Reuse, wenn schon vorhanden\n",
    "    if out_path.exists() and out_path.stat().st_size > 0:\n",
    "        return out_path\n",
    "\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-y\",\n",
    "        \"-i\", str(input_path),\n",
    "        \"-ac\", \"1\",        # mono\n",
    "        \"-ar\", \"16000\",    # 16kHz\n",
    "        \"-vn\",             # no video\n",
    "        str(out_path),\n",
    "    ]\n",
    "    res = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if res.returncode != 0:\n",
    "        raise RuntimeError(f\"ffmpeg failed:\\n{res.stderr}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Konvertierungs-Test\n",
    "# Testaudio mit normalisierter Version überschreiben\n",
    "conversion_test = True\n",
    "if conversion_test:\n",
    "    CONVERTED_DIR = REPO_ROOT / \"data\" / \"normalised_audio\"\n",
    "    TEST_AUDIO = ensure_wav_16k_mono(TEST_AUDIO, CONVERTED_DIR)\n",
    "\n",
    "    print(\"WAV:\", TEST_AUDIO)\n",
    "    print(\"Exists:\", TEST_AUDIO.exists(), \"Size:\", TEST_AUDIO.stat().st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "### vLLM-client, OpenAI-kompatibel\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def chat_vllm(messages, model=VLLM_MODEL, temperature=0.0, max_tokens=200, timeout=600):\n",
    "    url = f\"{VLLM_BASE_URL}/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    r = requests.post(url, json=payload, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    content = data[\"choices\"][0][\"message\"].get(\"content\", None)\n",
    "    if content is None:\n",
    "        raise RuntimeError(f\"LLM returned no content. Full response: {json.dumps(data)[:2000]}\")\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kurzer LLM-Test: Bei Bedarf auf True setzen\n",
    "llm_test = True\n",
    "\n",
    "if llm_test:\n",
    "    print(chat_vllm([{\"role\":\"user\",\"content\":\"Antworte nur mit Ja.\"}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147f205",
   "metadata": {},
   "source": [
    "Whisper-Transkription der Audio via faster-whisper\n",
    "- lokal reproduzierbar, keine Cloud\n",
    "- wir nehmen Segment-Zeitstempel für späteren Pyannote-Merge (nicht benötigt für LLM-Diarisierung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e6bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code für Transkription\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "WHISPER_MODEL_SIZE = os.getenv(\"WHISPER_MODEL_SIZE\", \"small\") # Kleines Modell für Tests\n",
    "# WHISPER_MODEL_SIZE = os.getenv(\"WHISPER_MODEL_SIZE\", \"large-v3\") # Großes Modell für Prod\n",
    "WHISPER_DEVICE = os.getenv(\"WHISPER_DEVICE\", \"cpu\")  # Für unseren Test auf CPU laufen lassen -> langsamer\n",
    "# WHISPER_DEVICE = os.getenv(\"WHISPER_DEVICE\", \"cuda\") # Wenn verfügbar: Auf GPU laufen lassen -> schneller -> Aber: Konfig-Anpassungen notwendig!\n",
    "WHISPER_COMPUTE_TYPE = os.getenv(\"WHISPER_COMPUTE_TYPE\", \"int8\")  # cpu: int8 gut\n",
    "\n",
    "_whisper_model = None\n",
    "\n",
    "def get_whisper_model():\n",
    "    global _whisper_model\n",
    "    if _whisper_model is None:\n",
    "        _whisper_model = WhisperModel(\n",
    "            WHISPER_MODEL_SIZE,\n",
    "            device=WHISPER_DEVICE,\n",
    "            compute_type=WHISPER_COMPUTE_TYPE,\n",
    "        )\n",
    "    return _whisper_model\n",
    "\n",
    "def transcribe_faster_whisper(audio_path: str, language: str | None = None):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      transcript: str (full text)\n",
    "      segments: list of dicts: [{\"start\": float, \"end\": float, \"text\": str}, ...]\n",
    "    \"\"\"\n",
    "    model = get_whisper_model()\n",
    "    segments_iter, info = model.transcribe(\n",
    "        audio_path,\n",
    "        language=language,\n",
    "        vad_filter=True,\n",
    "        word_timestamps=False,  # später ggf. True, falls wir word-level brauchen\n",
    "    )\n",
    "    segments = []\n",
    "    texts = []\n",
    "    for seg in segments_iter:\n",
    "        txt = seg.text.strip()\n",
    "        segments.append({\"start\": float(seg.start), \"end\": float(seg.end), \"text\": txt})\n",
    "        texts.append(txt)\n",
    "    transcript = \"\\n\".join(texts).strip()\n",
    "    return transcript, segments, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ccf199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper-Test anhand der Test-Audio. Aktivieren -> True setzen\n",
    "whisper_test = False\n",
    "if whisper_test:\n",
    "    t, segs, info = transcribe_faster_whisper(TEST_AUDIO, language=\"de\")\n",
    "    print(\"Language:\", info.language, \"Prob:\", info.language_probability)\n",
    "    print(\"Transcript:\\n\", t[:1000])\n",
    "    print(\"\\nFirst segments:\", segs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80cb3cc",
   "metadata": {},
   "source": [
    "### LLM-Diarisierung Prompt\n",
    "System- und User-Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f206fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarize_with_llm(transcript: str):\n",
    "    system_prompt = (\n",
    "        \"You are a professional conversation diarization engine. \"\n",
    "        \"Assign speaker labels logically solely based on the text. \"\n",
    "        \"No reasoning. \" # Test-Weise, weil sonst zu lange Zeiten.\n",
    "        \"Output only the diarized transcript.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Add speaker labels to the following transcript. \"\n",
    "        \"Use '[Sprecher 1]', '[Sprecher 2]', ... for the different speakers. \"\n",
    "        \"Check thoroughly for every sentence if the assignment to the speaker is contextually and logically plausible. \"\n",
    "        \"Produce a readable dialogue format between Speakers.\\n\\n\"\n",
    "        f\"{transcript}\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    return chat_vllm(messages, temperature=0.0, max_tokens=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fe6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kombinierter Test: Whisper-Diarisierung -> LLM-Diarisierung anhand der Test-Audio, True setzen für Test\n",
    "whisper_llm_test = True\n",
    "if whisper_llm_test:\n",
    "    transcript, segs, info = transcribe_faster_whisper(TEST_AUDIO, language=\"de\")\n",
    "    diarized = diarize_with_llm(transcript)\n",
    "    print(diarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1774b9eb",
   "metadata": {},
   "source": [
    "Batch-Loop für alle Audios\n",
    "- Whisper -> LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b159cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input-Audios vorbereiten\n",
    "from pathlib import Path\n",
    "\n",
    "BATCH_AUDIO_DIR = REPO_ROOT / \"data\" / \"input_audio\"\n",
    "BATCH_EXTS = {\".wav\", \".mp3\", \".m4a\", \".flac\", \".ogg\"}\n",
    "\n",
    "def list_audio_files(folder: Path) -> list[Path]:\n",
    "    files = []\n",
    "    for p in folder.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in BATCH_EXTS:\n",
    "            files.append(p)\n",
    "    return sorted(files)\n",
    "\n",
    "audio_files = list_audio_files(BATCH_AUDIO_DIR)\n",
    "print(\"Found files:\", len(audio_files))\n",
    "for f in audio_files[:10]:\n",
    "    print(\"-\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Laufzeit - Messung und korrektes Speichern, Hilfsfunktion\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "RESULTS_DIR = REPO_ROOT / \"results\"\n",
    "\n",
    "def save_run(out_dir: Path, *, meta: dict, transcript: str, diarized: str):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (out_dir / \"meta.json\").write_text(json.dumps(meta, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    (out_dir / \"transcript.txt\").write_text(transcript, encoding=\"utf-8\")\n",
    "    (out_dir / \"llm_diarized.txt\").write_text(diarized, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Haupt-Pipeline Batch-LLM-Diarisierung\n",
    "def run_whisper_llm_on_file(audio_path: Path, *, clip_chars: int | None = None) -> dict:\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 1) normalize\n",
    "    wav_path = ensure_wav_16k_mono(audio_path, REPO_ROOT / \"data\" / \"normalised_audio\")\n",
    "\n",
    "    # 2) transcribe\n",
    "    t1 = time.time()\n",
    "    transcript, segs, info = transcribe_faster_whisper(str(wav_path), language=\"de\")\n",
    "    t2 = time.time()\n",
    "\n",
    "    # 3) LLM diarize\n",
    "    diarized = diarize_with_llm(llm_input)\n",
    "    t3 = time.time()\n",
    "\n",
    "    meta = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"audio_path\": str(audio_path),\n",
    "        \"wav_path\": str(wav_path),\n",
    "        \"vllm_base_url\": VLLM_BASE_URL,\n",
    "        \"vllm_model\": VLLM_MODEL,\n",
    "        \"whisper_model_size\": WHISPER_MODEL_SIZE,\n",
    "        \"whisper_device\": WHISPER_DEVICE,\n",
    "        \"whisper_compute_type\": WHISPER_COMPUTE_TYPE,\n",
    "        \"transcript_chars\": len(transcript),\n",
    "        \"transcript_words\": len(transcript.split()),\n",
    "        \"llm_input_chars\": len(llm_input),\n",
    "        \"llm_output_chars\": len(diarized),\n",
    "        \"seconds_total\": round(t3 - t0, 2),\n",
    "        \"seconds_transcribe\": round(t2 - t1, 2),\n",
    "        \"seconds_llm\": round(t3 - t2, 2),\n",
    "        \"language\": getattr(info, \"language\", None),\n",
    "        \"language_probability\": float(getattr(info, \"language_probability\", 0.0) or 0.0),\n",
    "    }\n",
    "\n",
    "    # speichern\n",
    "    out_dir = RESULTS_DIR / audio_path.stem / VLLM_MODEL\n",
    "    save_run(out_dir, meta=meta, transcript=transcript, diarized=diarized)\n",
    "\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0811d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ausführen der Pipeline für alle Audios\n",
    "\n",
    "metas = []\n",
    "failures = []\n",
    "\n",
    "for i, f in enumerate(audio_files, 1):\n",
    "    print(f\"\\n[{i}/{len(audio_files)}] Processing:\", f)\n",
    "    try:\n",
    "        meta = run_whisper_llm_on_file(f, clip_chars=BATCH_CLIP_CHARS)\n",
    "        metas.append(meta)\n",
    "        print(\"  -> OK | total:\", meta[\"seconds_total\"], \"s | llm:\", meta[\"seconds_llm\"], \"s | chars:\", meta[\"transcript_chars\"])\n",
    "    except Exception as e:\n",
    "        failures.append({\"file\": str(f), \"error\": repr(e)})\n",
    "        print(\"  -> FAIL:\", repr(e))\n",
    "\n",
    "# summary files\n",
    "summary_dir = RESULTS_DIR / \"summaries\" / VLLM_MODEL\n",
    "summary_dir.mkdir(parents=True, exist_ok=True)\n",
    "(summary_dir / \"summary_meta.json\").write_text(json.dumps(metas, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "(summary_dir / \"failures.json\").write_text(json.dumps(failures, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "print(\"Success:\", len(metas), \"Failures:\", len(failures))\n",
    "print(\"Summary:\", summary_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
