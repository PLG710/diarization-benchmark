{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b951ed03",
   "metadata": {},
   "source": [
    "Pipeline A: Whisper -> LLM-Diarisierung\n",
    "Kein Chunking, kein JSON\n",
    "Output: [Sprecher 1]: ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports + Konfiguration\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# .env-Variablen (nicht öffentlich) laden\n",
    "VLLM_BASE_URL = os.getenv(\"VLLM_BASE_URL\")\n",
    "VLLM_MODEL = os.getenv(\"VLLM_MODEL\")\n",
    "REPO_ROOT = Path((os.getenv(\"REPO_ROOT\"))).expanduser().resolve()\n",
    "TEST_AUDIO = REPO_ROOT / os.getenv(\"TEST_AUDIO\") # In .env definierte Audio-Datei für einzelne Tests\n",
    "\n",
    "# Pfad zu gespeicherten Audios\n",
    "AUDIO_DIR = REPO_ROOT / \"data\" / \"input_audio\"\n",
    "CONVERTED_DIR = REPO_ROOT / \"data\" / \"normalised_audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Audio-Normalisierung via ffmpeg, alles konvertieren zu WAV 16khz Mono -> Wichtig für Pyannote\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_wav_16k_mono(input_path: str | Path, out_dir: str | Path) -> Path:\n",
    "    input_path = Path(input_path)\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_path = out_dir / f\"{input_path.stem}_16k_mono.wav\"\n",
    "\n",
    "    # Reuse, wenn schon vorhanden\n",
    "    if out_path.exists() and out_path.stat().st_size > 0:\n",
    "        return out_path\n",
    "\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-y\",\n",
    "        \"-i\", str(input_path),\n",
    "        \"-ac\", \"1\",        # mono\n",
    "        \"-ar\", \"16000\",    # 16kHz\n",
    "        \"-vn\",             # no video\n",
    "        str(out_path),\n",
    "    ]\n",
    "    res = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if res.returncode != 0:\n",
    "        raise RuntimeError(f\"ffmpeg failed:\\n{res.stderr}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Konvertierungs-Test\n",
    "# Testaudio mit normalisierter Version überschreiben\n",
    "conversion_test = False\n",
    "if conversion_test:\n",
    "    TEST_AUDIO = ensure_wav_16k_mono(TEST_AUDIO, CONVERTED_DIR)\n",
    "\n",
    "    print(\"WAV:\", TEST_AUDIO)\n",
    "    print(\"Exists:\", TEST_AUDIO.exists(), \"Size:\", TEST_AUDIO.stat().st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "### vLLM-Client\n",
    "\n",
    "def chat_vllm(messages, model=VLLM_MODEL, temperature=0.0, max_tokens=200, timeout=600):\n",
    "    url = f\"{VLLM_BASE_URL}/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    r = requests.post(url, json=payload, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    content = data[\"choices\"][0][\"message\"].get(\"content\", None)\n",
    "    if content is None:\n",
    "        raise RuntimeError(f\"LLM returned no content. Full response: {json.dumps(data)[:2000]}\")\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kurzer LLM-Test: Bei Bedarf auf True setzen\n",
    "llm_test = True\n",
    "\n",
    "if llm_test:\n",
    "    print(chat_vllm([{\"role\":\"user\",\"content\":\"Antworte nur mit OK\"}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147f205",
   "metadata": {},
   "source": [
    "Whisper-Transkription der Audio via faster-whisper\n",
    "- lokal reproduzierbar, keine Cloud\n",
    "- wir nehmen Segment-Zeitstempel für späteren Pyannote-Merge (nicht benötigt für LLM-Diarisierung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e6bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Whisper-Config\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "WHISPER_MODEL_SIZE = os.getenv(\"WHISPER_MODEL_SIZE\", \"small\") # Kleines Modell für Tests\n",
    "# WHISPER_MODEL_SIZE = os.getenv(\"WHISPER_MODEL_SIZE\", \"large-v3\") # Großes Modell für Prod\n",
    "WHISPER_DEVICE = os.getenv(\"WHISPER_DEVICE\", \"cpu\")  # Für unseren Test auf CPU laufen lassen -> langsamer\n",
    "# WHISPER_DEVICE = os.getenv(\"WHISPER_DEVICE\", \"cuda\") # Wenn verfügbar: Auf GPU laufen lassen -> schneller -> Aber: Konfig-Anpassungen notwendig!\n",
    "WHISPER_COMPUTE_TYPE = os.getenv(\"WHISPER_COMPUTE_TYPE\", \"int8\")  # cpu: int8 gut\n",
    "\n",
    "whisper_model = WhisperModel(WHISPER_MODEL_SIZE, device=WHISPER_DEVICE, compute_type=WHISPER_COMPUTE_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad921a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transkription\n",
    "def transcribe_faster_whisper(audio_path: str, language: str | None = None):\n",
    "    segments_iter, info = whisper_model.transcribe(\n",
    "        audio_path,\n",
    "        language=language,\n",
    "        vad_filter=True,\n",
    "        word_timestamps=False,  # später ggf. True, falls wir word-level brauchen\n",
    "    )\n",
    "    segments = []\n",
    "    texts = []\n",
    "    for seg in segments_iter:\n",
    "        txt = seg.text.strip()\n",
    "        segments.append({\"start\": float(seg.start), \"end\": float(seg.end), \"text\": txt})\n",
    "        texts.append(txt)\n",
    "    transcript = \"\\n\".join(texts).strip()\n",
    "    return transcript, segments, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ccf199",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Whisper-Test anhand der Test-Audio. Aktivieren -> True setzen\n",
    "whisper_test = False\n",
    "if whisper_test:\n",
    "    t, segs, info = transcribe_faster_whisper(TEST_AUDIO, language=\"de\")\n",
    "    print(\"Language:\", info.language, \"Prob:\", info.language_probability)\n",
    "    print(\"Transcript:\\n\", t[:1000])\n",
    "    print(\"\\nFirst segments:\", segs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80cb3cc",
   "metadata": {},
   "source": [
    "### LLM-Diarisierung Prompt\n",
    "System- und User-Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f206fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarize_with_llm(transcript: str):\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a professional conversation diarization engine. \"\n",
    "        \"Assign speaker labels logically solely based on the text. \"\n",
    "        \"No reasoning. \" # Test-Weise, weil sonst zu lange Zeiten.\n",
    "        \"Output only the diarized transcript.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Add speaker labels to the following transcript. \"\n",
    "        \"Use '[Sprecher 1]', '[Sprecher 2]', ... for the different speakers. \"\n",
    "        \"Check thoroughly for every sentence if the assignment to the speaker is contextually and logically plausible. \"\n",
    "        \"Produce a readable dialogue format between Speakers.\\n\\n\"\n",
    "        f\"{transcript}\"\n",
    "    )\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    return chat_vllm(messages, temperature=0.0, max_tokens=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fe6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kombinierter Test: Whisper-Diarisierung -> LLM-Diarisierung anhand der Test-Audio, True setzen für Test\n",
    "whisper_llm_test = False\n",
    "if whisper_llm_test:\n",
    "    print(\"Starte Transkription mit faster-whisper\")\n",
    "    t0 = time.time()\n",
    "    transcript, segs, info = transcribe_faster_whisper(TEST_AUDIO, language=\"de\")\n",
    "    t1 = time.time()\n",
    "    print(f\"Whisper-Transkription abgeschlossen in ({t1 - t0:.2f}s)\")\n",
    "    print(\"LLM-Diarisierung gestartet\")\n",
    "    t2 = time.time()\n",
    "    diarized = diarize_with_llm(transcript)\n",
    "    t3 = time.time()\n",
    "    print(f\"LLM-Diarisierung abgeschlossen in ({t3 - t2:.2f}s)\")\n",
    "    print(diarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pyannote-Setup (CPU-only)\n",
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "assert HF_TOKEN, \"HUGGINGFACE_TOKEN fehlt in .env\" # Fehlerbehandlung\n",
    "\n",
    "PYANNOTE_MODEL_ID = os.getenv(\"PYANNOTE_MODEL_ID\")\n",
    "\n",
    "pyannote_pipeline = Pipeline.from_pretrained(PYANNOTE_MODEL_ID, token=HF_TOKEN)\n",
    "\n",
    "# CPU erzwingen\n",
    "pyannote_pipeline.to(torch.device(\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pyannote starten\n",
    "def run_pyannote(wav_path: Path):\n",
    "    \n",
    "    diarization = pyannote_pipeline(str(wav_path))\n",
    "\n",
    "    return diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af7b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Minimales Pyannote Post-Processing (Mapping auf die Whisper-Segmente)\n",
    "import re\n",
    "\n",
    "### Pipeline: ensure_wav_16k_mono -> transcribe_faster_whisper -> run_pyannote -> label_whisper_segments -> normalize_speakers \n",
    "### (für Lesbarkeit, optional) -> merge_same_speakers (für Lesbarkeit, optional) -> format_dialogue (optional?)\n",
    "\n",
    "def pyannote_turns(diarization):\n",
    "    # diarization kann Annotation sein oder ein Objekt mit .speaker_diarization (Letzteres in unserem Fall)\n",
    "    ann = getattr(diarization, \"speaker_diarization\", diarization)\n",
    "    turns = []\n",
    "    for turn, _, speaker in ann.itertracks(yield_label=True):\n",
    "        turns.append((float(turn.start), float(turn.end), str(speaker)))\n",
    "    turns.sort(key=lambda x: (x[0], x[1]))\n",
    "    return turns\n",
    "\n",
    "def overlap(a0, a1, b0, b1):\n",
    "    return max(0.0, min(a1, b1) - max(a0, b0))\n",
    "\n",
    "# Haupt-Funktion zum Mappen, ohne Start- und End-Zeiten\n",
    "def label_whisper_segments(segments, diarization, min_overlap_s=0.05):\n",
    "\n",
    "    turns = pyannote_turns(diarization)\n",
    "    labeled = []\n",
    "\n",
    "    for seg in segments:\n",
    "        s0, s1 = float(seg[\"start\"]), float(seg[\"end\"])\n",
    "        best_spk, best_ol = \"UNK\", 0.0\n",
    "\n",
    "        for t0, t1, spk in turns:\n",
    "            if t1 <= s0:\n",
    "                continue\n",
    "            if t0 >= s1:\n",
    "                break\n",
    "            ol = overlap(s0, s1, t0, t1)\n",
    "            if ol > best_ol:\n",
    "                best_ol, best_spk = ol, spk\n",
    "\n",
    "        if best_ol < min_overlap_s:\n",
    "            best_spk = \"UNK\"\n",
    "\n",
    "        labeled.append({\n",
    "            \"start\": s0, \"end\": s1,              # intern behalten\n",
    "            \"speaker_raw\": best_spk,\n",
    "            \"text\": (seg.get(\"text\") or \"\").strip()\n",
    "        })\n",
    "\n",
    "    return labeled\n",
    "\n",
    "### Kosmetische Funktion zur Vergleichbarkeit mit LLM-Ausgabe SPEAKER_00 -> Sprecher 1\n",
    "def normalize_speakers(labeled):\n",
    "    # rein kosmetisch: SPEAKER_00 -> Sprecher 1, ...\n",
    "    mapping = {}\n",
    "    n = 1\n",
    "    for x in labeled:\n",
    "        raw = x[\"speaker_raw\"]\n",
    "        if raw != \"UNK\" and raw not in mapping:\n",
    "            mapping[raw] = f\"[Sprecher {n}]\"\n",
    "            n += 1\n",
    "    for x in labeled:\n",
    "        x[\"speaker\"] = mapping.get(x[\"speaker_raw\"], \"UNK\")\n",
    "    return labeled\n",
    "\n",
    "def merge_same_speaker(labeled, max_gap_s=0.8):\n",
    "    # nur Lesbarkeit: aufeinanderfolgende Segmente gleichen Speakers zusammenziehen\n",
    "    # (Gar kein Heuristik-Merging: max_gap_s=0 setzen)\n",
    "    out = []\n",
    "    for seg in labeled:\n",
    "        if not seg[\"text\"]:\n",
    "            continue\n",
    "        if not out:\n",
    "            out.append(dict(seg))\n",
    "            continue\n",
    "\n",
    "        prev = out[-1]\n",
    "        gap = seg[\"start\"] - prev[\"end\"]\n",
    "        if seg[\"speaker\"] == prev[\"speaker\"] and gap <= max_gap_s:\n",
    "            prev[\"text\"] = (prev[\"text\"].rstrip() + \" \" + seg[\"text\"].lstrip()).strip()\n",
    "            prev[\"end\"] = seg[\"end\"]\n",
    "        else:\n",
    "            out.append(dict(seg))\n",
    "    return out\n",
    "\n",
    "def format_dialogue(utterances):\n",
    "    lines = []\n",
    "    for u in utterances:\n",
    "        txt = re.sub(r\"\\s+\", \" \", u[\"text\"]).strip()\n",
    "        if txt:\n",
    "            lines.append(f\"{u['speaker']}: {txt}\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e8db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pyannote-Test mit vorheriger Transkribierung (für Segmente), bei Bedarf auf True setzen\n",
    "pyannote_test = True\n",
    "if pyannote_test:\n",
    "    print(\"Normalisiere Datei für Pyannote (wav-Datei mit 16k mono)\")\n",
    "    wav_path = ensure_wav_16k_mono(TEST_AUDIO, CONVERTED_DIR)\n",
    "    print(\"Normalisierung abgeschlossen\")\n",
    "    print(\"Starte Transkription mit faster-whisper\")\n",
    "    t0 = time.time()\n",
    "    transcript, segs, info = transcribe_faster_whisper(wav_path, language=\"de\")\n",
    "    t1 = time.time()    \n",
    "    print(f\"Whisper-Transkription abgeschlossen in ({t1 - t0:.2f}s)\")\n",
    "    print(\"Pyannote gestartet...\")\n",
    "    t2 = time.time()\n",
    "    diarization = run_pyannote(wav_path)\n",
    "    t3 = time.time()\n",
    "    print(f\"Pyannote abgeschlossen in ({t3 - t2:.2f}s)\")\n",
    "    print(\"Text-Aufbereitung begonnen\")\n",
    "    t4 = time.time()\n",
    "    # alle Schritte als eigene Variable speichern für Debugging\n",
    "    labeled = label_whisper_segments(segs, diarization, min_overlap_s=0.05) # 0.05 verbreiteter Default-Wert\n",
    "    labeled2 = normalize_speakers(labeled)\n",
    "    labeled3 = merge_same_speaker(labeled2, max_gap_s=0.8)\n",
    "    labeled4 = format_dialogue(labeled3)\n",
    "    t5 = time.time()\n",
    "    print(f\"Nachbearbeitung abgeschlossen in ({t5-t4:.2f}s)\")\n",
    "    print(f\"Gesamt-Pipeline (Pyannote + Nachbereitung) abgeschlossen in ({t5-t2:.2f}s)\")\n",
    "\n",
    "    print(\"End-Ausgabe:\\n\" + labeled4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1774b9eb",
   "metadata": {},
   "source": [
    "Batch-Loop für alle Audios\n",
    "- Whisper -> LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b159cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input-Audios vorbereiten\n",
    "\n",
    "BATCH_AUDIO_DIR = REPO_ROOT / \"data\" / \"input_audio\"\n",
    "BATCH_EXTS = {\".wav\", \".mp3\", \".m4a\", \".flac\", \".ogg\"}\n",
    "\n",
    "def list_audio_files(folder: Path) -> list[Path]:\n",
    "    files = []\n",
    "    for p in folder.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in BATCH_EXTS:\n",
    "            files.append(p)\n",
    "    return sorted(files)\n",
    "\n",
    "audio_files = list_audio_files(BATCH_AUDIO_DIR)\n",
    "print(\"Found files:\", len(audio_files))\n",
    "for f in audio_files[:10]:\n",
    "    print(\"-\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Laufzeit - Messung und korrektes Speichern, Hilfsfunktion\n",
    "RESULTS_DIR = REPO_ROOT / \"results\"\n",
    "\n",
    "def save_run(out_dir: Path, *, meta: dict, transcript: str, diarized: str):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (out_dir / \"meta.json\").write_text(json.dumps(meta, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    (out_dir / \"transcript.txt\").write_text(transcript, encoding=\"utf-8\")\n",
    "    (out_dir / \"llm_diarized.txt\").write_text(diarized, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Haupt-Pipeline Batch-LLM-Diarisierung\n",
    "def run_whisper_llm_on_file(audio_path: Path, *, clip_chars: int | None = None) -> dict:\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 1) normalize\n",
    "    wav_path = ensure_wav_16k_mono(audio_path, REPO_ROOT / \"data\" / \"normalised_audio\")\n",
    "\n",
    "    # 2) transcribe\n",
    "    t1 = time.time()\n",
    "    transcript, segs, info = transcribe_faster_whisper(str(wav_path), language=\"de\")\n",
    "    t2 = time.time()\n",
    "\n",
    "    # 3) LLM diarize\n",
    "    diarized = diarize_with_llm(llm_input)\n",
    "    t3 = time.time()\n",
    "\n",
    "    meta = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"audio_path\": str(audio_path),\n",
    "        \"wav_path\": str(wav_path),\n",
    "        \"vllm_base_url\": VLLM_BASE_URL,\n",
    "        \"vllm_model\": VLLM_MODEL,\n",
    "        \"whisper_model_size\": WHISPER_MODEL_SIZE,\n",
    "        \"whisper_device\": WHISPER_DEVICE,\n",
    "        \"whisper_compute_type\": WHISPER_COMPUTE_TYPE,\n",
    "        \"transcript_chars\": len(transcript),\n",
    "        \"transcript_words\": len(transcript.split()),\n",
    "        \"llm_input_chars\": len(llm_input),\n",
    "        \"llm_output_chars\": len(diarized),\n",
    "        \"seconds_total\": round(t3 - t0, 2),\n",
    "        \"seconds_transcribe\": round(t2 - t1, 2),\n",
    "        \"seconds_llm\": round(t3 - t2, 2),\n",
    "        \"language\": getattr(info, \"language\", None),\n",
    "        \"language_probability\": float(getattr(info, \"language_probability\", 0.0) or 0.0),\n",
    "    }\n",
    "\n",
    "    # speichern\n",
    "    out_dir = RESULTS_DIR / audio_path.stem / VLLM_MODEL\n",
    "    save_run(out_dir, meta=meta, transcript=transcript, diarized=diarized)\n",
    "\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0811d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ausführen der Pipeline für alle Audios\n",
    "\n",
    "metas = []\n",
    "failures = []\n",
    "\n",
    "for i, f in enumerate(audio_files, 1):\n",
    "    print(f\"\\n[{i}/{len(audio_files)}] Processing:\", f)\n",
    "    try:\n",
    "        meta = run_whisper_llm_on_file(f, clip_chars=BATCH_CLIP_CHARS)\n",
    "        metas.append(meta)\n",
    "        print(\"  -> OK | total:\", meta[\"seconds_total\"], \"s | llm:\", meta[\"seconds_llm\"], \"s | chars:\", meta[\"transcript_chars\"])\n",
    "    except Exception as e:\n",
    "        failures.append({\"file\": str(f), \"error\": repr(e)})\n",
    "        print(\"  -> FAIL:\", repr(e))\n",
    "\n",
    "# summary files\n",
    "summary_dir = RESULTS_DIR / \"summaries\" / VLLM_MODEL\n",
    "summary_dir.mkdir(parents=True, exist_ok=True)\n",
    "(summary_dir / \"summary_meta.json\").write_text(json.dumps(metas, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "(summary_dir / \"failures.json\").write_text(json.dumps(failures, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "print(\"Success:\", len(metas), \"Failures:\", len(failures))\n",
    "print(\"Summary:\", summary_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
