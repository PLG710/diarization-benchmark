{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b951ed03",
   "metadata": {},
   "source": [
    "Pipeline A: Whisper -> LLM-Diarisierung\n",
    "Kein Chunking, kein JSON\n",
    "Output: [Sprecher 1]: ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba28719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports + Konfiguration\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "VLLM_BASE_URL = os.getenv(\"VLLM_BASE_URL\", \"http://127.0.0.1:8005/v1\")\n",
    "VLLM_MODEL = os.getenv(\"VLLM_MODEL\", \"openai/gpt-oss-120b\")\n",
    "\n",
    "# Repository-Root: individuell anpassen\n",
    "REPO_ROOT = Path(\"~/jupyter/diarization-benchmark\").expanduser().resolve()\n",
    "\n",
    "# Pfad zu gespeicherten Audios\n",
    "AUDIO_DIR = REPO_ROOT / \"data\" / \"input_audio\"\n",
    "\n",
    "# Test-Audio-Datei (Benennen mit test1.mp3, ggf. anpassen)\n",
    "TEST_AUDIO = REPO_ROOT /\"data\" / \"test_audio\" / \"test1.mp3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d7d5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Audio-Normalisierung via ffmpeg, alles konvertieren zu WAV 16khz Mono -> Wichtig für Pyannote\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_wav_16k_mono(input_path: str | Path, out_dir: str | Path) -> Path:\n",
    "    input_path = Path(input_path)\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_path = out_dir / f\"{input_path.stem}_16k_mono.wav\"\n",
    "\n",
    "    # Reuse, wenn schon vorhanden\n",
    "    if out_path.exists() and out_path.stat().st_size > 0:\n",
    "        return out_path\n",
    "\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-y\",\n",
    "        \"-i\", str(input_path),\n",
    "        \"-ac\", \"1\",        # mono\n",
    "        \"-ar\", \"16000\",    # 16kHz\n",
    "        \"-vn\",             # no video\n",
    "        str(out_path),\n",
    "    ]\n",
    "    res = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if res.returncode != 0:\n",
    "        raise RuntimeError(f\"ffmpeg failed:\\n{res.stderr}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01b1faac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAV: /home/liegepa/jupyter/diarization-benchmark/data/normalised_audio/test1_16k_mono.wav\n",
      "Exists: True Size: 34685862\n"
     ]
    }
   ],
   "source": [
    "### Konvertierungs-Test\n",
    "# Testaudio mit normalisierter Version überschreiben\n",
    "conversion_test = True\n",
    "if conversion_test:\n",
    "    CONVERTED_DIR = REPO_ROOT / \"data\" / \"normalised_audio\"\n",
    "    TEST_AUDIO = ensure_wav_16k_mono(TEST_AUDIO, CONVERTED_DIR)\n",
    "\n",
    "    print(\"WAV:\", TEST_AUDIO)\n",
    "    print(\"Exists:\", TEST_AUDIO.exists(), \"Size:\", TEST_AUDIO.stat().st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d20b9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "### vLLM-client, OpenAI-kompatibel\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def chat_vllm(messages, model=VLLM_MODEL, temperature=0.0, max_tokens=800, timeout=600):\n",
    "    url = f\"{VLLM_BASE_URL}/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    r = requests.post(url, json=payload, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    content = data[\"choices\"][0][\"message\"].get(\"content\", None)\n",
    "    if content is None:\n",
    "        raise RuntimeError(f\"LLM returned no content. Full response: {json.dumps(data)[:2000]}\")\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c076cc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ja\n"
     ]
    }
   ],
   "source": [
    "# Kurzer LLM-Test: Bei Bedarf auf True setzen\n",
    "llm_test = True\n",
    "\n",
    "if llm_test:\n",
    "    print(chat_vllm([{\"role\":\"user\",\"content\":\"Antworte nur mit Ja.\"}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147f205",
   "metadata": {},
   "source": [
    "Whisper-Transkription der Audio via faster-whisper\n",
    "- lokal reproduzierbar, keine Cloud\n",
    "- wir nehmen Segment-Zeitstempel für späteren Pyannote-Merge (nicht benötigt für LLM-Diarisierung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49e6bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code für Transkription\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "WHISPER_MODEL_SIZE = os.getenv(\"WHISPER_MODEL_SIZE\", \"small\") # Kleines Modell für Tests\n",
    "# WHISPER_MODEL_SIZE = os.getenv(\"WHISPER_MODEL_SIZE\", \"large-v3\") # Großes Modell für Prod\n",
    "WHISPER_DEVICE = os.getenv(\"WHISPER_DEVICE\", \"cpu\")  # Für unseren Test auf CPU laufen lassen -> langsamer\n",
    "# WHISPER_DEVICE = os.getenv(\"WHISPER_DEVICE\", \"cuda\") # Wenn verfügbar: Auf GPU laufen lassen -> schneller -> Aber: Konfig-Anpassungen notwendig!\n",
    "WHISPER_COMPUTE_TYPE = os.getenv(\"WHISPER_COMPUTE_TYPE\", \"int8\")  # cpu: int8 gut\n",
    "\n",
    "_whisper_model = None\n",
    "\n",
    "def get_whisper_model():\n",
    "    global _whisper_model\n",
    "    if _whisper_model is None:\n",
    "        _whisper_model = WhisperModel(\n",
    "            WHISPER_MODEL_SIZE,\n",
    "            device=WHISPER_DEVICE,\n",
    "            compute_type=WHISPER_COMPUTE_TYPE,\n",
    "        )\n",
    "    return _whisper_model\n",
    "\n",
    "def transcribe_faster_whisper(audio_path: str, language: str | None = None):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      transcript: str (full text)\n",
    "      segments: list of dicts: [{\"start\": float, \"end\": float, \"text\": str}, ...]\n",
    "    \"\"\"\n",
    "    model = get_whisper_model()\n",
    "    segments_iter, info = model.transcribe(\n",
    "        audio_path,\n",
    "        language=language,\n",
    "        vad_filter=True,\n",
    "        word_timestamps=False,  # später ggf. True, falls wir word-level brauchen\n",
    "    )\n",
    "    segments = []\n",
    "    texts = []\n",
    "    for seg in segments_iter:\n",
    "        txt = seg.text.strip()\n",
    "        segments.append({\"start\": float(seg.start), \"end\": float(seg.end), \"text\": txt})\n",
    "        texts.append(txt)\n",
    "    transcript = \"\\n\".join(texts).strip()\n",
    "    return transcript, segments, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75ccf199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper-Test anhand der Test-Audio. Aktivieren -> True setzen\n",
    "whisper_test = False\n",
    "if whisper_test:\n",
    "    t, segs, info = transcribe_faster_whisper(TEST_AUDIO, language=\"de\")\n",
    "    print(\"Language:\", info.language, \"Prob:\", info.language_probability)\n",
    "    print(\"Transcript:\\n\", t[:1000])\n",
    "    print(\"\\nFirst segments:\", segs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80cb3cc",
   "metadata": {},
   "source": [
    "### LLM-Diarisierung Prompt\n",
    "System- und User-Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f206fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarize_with_llm(transcript: str):\n",
    "    system_prompt = (\n",
    "        \"You are a professional conversation diarization engine. \"\n",
    "        \"Assign speaker labels logically solely based on text. \"\n",
    "        \"Do not add, remove, or summarize content. \"\n",
    "        \"Make sure before assignment that the context fits (a doctor will not say things a patient would say for example). \"\n",
    "        \"Output only the diarized transcript.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Add speaker labels to the following transcript using logic to determine which sentence belongs to which speaker. \"\n",
    "        \"For assigning sentences: Make out the role of the speaker (for example doctor). Keep this in mind when assigning speaker names. \"\n",
    "        \"Use '[Sprecher 1]', '[Sprecher 2]', ... for the different speakers. \"\n",
    "        \"Check thoroughly for every sentence if the assignment to the speaker is contextually and logically plausible. \"\n",
    "        \"For example, in a doctor-patient-dialogue, do not assign medical questions to the Speaker who was identified as a patient before. \"\n",
    "        \"Produce a readable dialogue format between Speakers.\\n\\n\"\n",
    "        f\"{transcript}\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    return chat_vllm(messages, temperature=0.0, max_tokens=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fe6b3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m whisper_llm_test:\n\u001b[32m      4\u001b[39m     transcript, segs, info = transcribe_faster_whisper(TEST_AUDIO, language=\u001b[33m\"\u001b[39m\u001b[33mde\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     diarized = \u001b[43mdiarize_with_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscript\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(diarized)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mdiarize_with_llm\u001b[39m\u001b[34m(transcript)\u001b[39m\n\u001b[32m     10\u001b[39m user_prompt = (\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAdd speaker labels to the following transcript using logic to determine which sentence belongs to which speaker. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFor assigning sentences: Make out the role of the speaker (for example doctor). Keep this in mind when assigning speaker names. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranscript\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m messages = [\n\u001b[32m     21\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: system_prompt},\n\u001b[32m     22\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: user_prompt},\n\u001b[32m     23\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchat_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mchat_vllm\u001b[39m\u001b[34m(messages, model, temperature, max_tokens, timeout)\u001b[39m\n\u001b[32m      6\u001b[39m url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVLLM_BASE_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m payload = {\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m     12\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m r = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m r.raise_for_status()\n\u001b[32m     15\u001b[39m data = r.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter/diarization-benchmark/.venv/lib/python3.12/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter/diarization-benchmark/.venv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter/diarization-benchmark/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter/diarization-benchmark/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter/diarization-benchmark/.venv/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter/diarization-benchmark/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter/diarization-benchmark/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter/diarization-benchmark/.venv/lib/python3.12/site-packages/urllib3/connection.py:571\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    568\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    570\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    574\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1448\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1446\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1447\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1450\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:336\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    338\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:297\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    299\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Kombinierter Test: Whisper-Diarisierung -> LLM-Diarisierung anhand der Test-Audio, True setzen für Test\n",
    "whisper_llm_test = True\n",
    "if whisper_llm_test:\n",
    "    transcript, segs, info = transcribe_faster_whisper(TEST_AUDIO, language=\"de\")\n",
    "    print(\"Chars:\", len(transcript))\n",
    "    print(\"Words:\", len(transcript.split()))\n",
    "    diarized = diarize_with_llm(transcript[:3000])\n",
    "    print(diarized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
