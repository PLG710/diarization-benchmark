{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b951ed03",
   "metadata": {},
   "source": [
    "Pipeline A: Whisper -> LLM-Diarisierung\n",
    "Kein Chunking, kein JSON\n",
    "Output: [Sprecher 1]: ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba28719f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_ROOT: /home/liegepa/jupyter/diarization-benchmark\n",
      "TEST_AUDIO exists: True /home/liegepa/jupyter/diarization-benchmark/data/test_audio/test1.mp3\n"
     ]
    }
   ],
   "source": [
    "### Imports + Konfiguration\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "VLLM_BASE_URL = os.getenv(\"VLLM_BASE_URL\", \"http://127.0.0.1:8005/v1\")\n",
    "VLLM_MODEL = os.getenv(\"VLLM_MODEL\", \"openai/gpt-oss-120b\")\n",
    "\n",
    "# Repository-Root: individuell anpassen\n",
    "REPO_ROOT = Path(\"~/jupyter/diarization-benchmark\").expanduser().resolve()\n",
    "\n",
    "# Pfad zu gespeicherten Audios\n",
    "AUDIO_DIR = REPO_ROOT / \"data\" / \"audio\"\n",
    "\n",
    "# Test-Audio-Datei (kann später weg), Endung beliebig -> normalisieren es später\n",
    "TEST_AUDIO = REPO_ROOT /\"data\" / \"test_audio\" / \"test1.mp3\"\n",
    "\n",
    "# Path-Debugging\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"TEST_AUDIO exists:\", TEST_AUDIO.exists(), TEST_AUDIO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d20b9707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ja\n"
     ]
    }
   ],
   "source": [
    "### vLLM-client, OpenAI-kompatibel\n",
    "import requests\n",
    "\n",
    "def chat_vllm(messages, model=VLLM_MODEL, temperature=0.0, max_tokens=800):\n",
    "    url = f\"{VLLM_BASE_URL}/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    data = requests.post(url, json=payload, timeout=600).json()\n",
    "    return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Erster Test\n",
    "print(chat_vllm([{\"role\":\"user\",\"content\":\"Antworte nur mit Ja.\"}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147f205",
   "metadata": {},
   "source": [
    "Whisper-Transkription der Audio via faster-whisper\n",
    "- lokal reproduzierbar, keine Cloud\n",
    "- wir nehmen Segment-Zeitstempel für späteren Pyannote-Merge (nicht benötigt für LLM-Diarisierung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e6bb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: de Prob: 1\n",
      "Transcript:\n",
      " Guten Tag, Krilloffice, mein Name ist Ernst Harz, sagen Sie mir bitte, heißen Sie?\n",
      "Ich heiße Brinkmann Maria.\n",
      "Frau Brinkmann, könnten Sie bitte Ihren Namen langsam Buch stabilieren?\n",
      "B-R-I-N-K-M-A-Doppel-N.\n",
      "Okay, und der Vorname?\n",
      "Maria M-A-R-I-A.\n",
      "Gut, Frau Brinkmann, wann sind Sie geboren und wie alt sind Sie?\n",
      "Ich bin am 15.15.1972 geboren und das heißt, ich bin 47 Jahre alt.\n",
      "Prima.\n",
      "Frau Brinkmann, sagen Sie bitte, was für eine Sitzung ist?\n",
      "Ja, Herr Doktor, ich habe so einen Schmerzen hier im Handgelenk und in meinen Sprunggelenk.\n",
      "Ich war auf dem Weg zur Arbeit mit meinem Fahrrad heute Morgen\n",
      "und dann war mir ein bisschen schwindelig\n",
      "und dann bin ich mit dem Fahrrad gestürzt.\n",
      "Und ich bin so auf meiner, wie heißt das?\n",
      "Ich bin auf meiner rechten Hand, so mit ausgestrecktem Hand, so auf dem Boden gefallen\n",
      "und mein Sprunggelenk ist auch umgeknippt während des Umfalls.\n",
      "Gut, also Frau Brinkmann, Sie sagen, Sie haben jetzt momentan Schmerzen,\n",
      "brauchen Sie jetzt ein Schmerzmittel?\n",
      "Ich kann die \n",
      "\n",
      "First segments: [{'start': 0.05, 'end': 5.05, 'text': 'Guten Tag, Krilloffice, mein Name ist Ernst Harz, sagen Sie mir bitte, heißen Sie?'}, {'start': 5.05, 'end': 7.05, 'text': 'Ich heiße Brinkmann Maria.'}, {'start': 7.05, 'end': 11.05, 'text': 'Frau Brinkmann, könnten Sie bitte Ihren Namen langsam Buch stabilieren?'}]\n"
     ]
    }
   ],
   "source": [
    "### Code für Transkription\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "WHISPER_MODEL_SIZE = os.getenv(\"WHISPER_MODEL_SIZE\", \"small\") # Kleines Modell für Tests\n",
    "# WHISPER_MODEL_SIZE = os.getenv(\"WHISPER_MODEL_SIZE\", \"large-v3\") # Großes Modell für Prod\n",
    "WHISPER_DEVICE = os.getenv(\"WHISPER_DEVICE\", \"cpu\")  # Für unseren Test auf CPU laufen lassen -> langsamer\n",
    "# WHISPER_DEVICE = os.getenv(\"WHISPER_DEVICE\", \"cuda\") # Wenn verfügbar: Auf GPU laufen lassen -> schneller -> Aber: Konfig-Anpassungen notwendig!\n",
    "WHISPER_COMPUTE_TYPE = os.getenv(\"WHISPER_COMPUTE_TYPE\", \"int8\")  # cpu: int8 gut\n",
    "\n",
    "_whisper_model = None\n",
    "\n",
    "def get_whisper_model():\n",
    "    global _whisper_model\n",
    "    if _whisper_model is None:\n",
    "        _whisper_model = WhisperModel(\n",
    "            WHISPER_MODEL_SIZE,\n",
    "            device=WHISPER_DEVICE,\n",
    "            compute_type=WHISPER_COMPUTE_TYPE,\n",
    "        )\n",
    "    return _whisper_model\n",
    "\n",
    "def transcribe_faster_whisper(audio_path: str, language: str | None = None):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      transcript: str (full text)\n",
    "      segments: list of dicts: [{\"start\": float, \"end\": float, \"text\": str}, ...]\n",
    "    \"\"\"\n",
    "    model = get_whisper_model()\n",
    "    segments_iter, info = model.transcribe(\n",
    "        audio_path,\n",
    "        language=language,\n",
    "        vad_filter=True,\n",
    "        word_timestamps=False,  # später ggf. True, falls wir word-level brauchen\n",
    "    )\n",
    "    segments = []\n",
    "    texts = []\n",
    "    for seg in segments_iter:\n",
    "        txt = seg.text.strip()\n",
    "        segments.append({\"start\": float(seg.start), \"end\": float(seg.end), \"text\": txt})\n",
    "        texts.append(txt)\n",
    "    transcript = \"\\n\".join(texts).strip()\n",
    "    return transcript, segments, info\n",
    "\n",
    "# Quick test, wenn TEST_AUDIO gesetzt ist\n",
    "if TEST_AUDIO:\n",
    "    t, segs, info = transcribe_faster_whisper(TEST_AUDIO, language=\"de\")\n",
    "    print(\"Language:\", info.language, \"Prob:\", info.language_probability)\n",
    "    print(\"Transcript:\\n\", t[:1000])\n",
    "    print(\"\\nFirst segments:\", segs[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80cb3cc",
   "metadata": {},
   "source": [
    "### LLM-Diarisierung Prompt\n",
    "System- und User-Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f206fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def diarize_with_llm(transcript: str):\n",
    "    system_prompt = (\n",
    "        \"You are a professional conversation diarization engine. \"\n",
    "        \"Assign speaker labels logically solely based on text. \"\n",
    "        \"Do not add, remove, or summarize content. \"\n",
    "        \"Make sure before assignment that the context fits (a doctor will not say things a patient would say for example). \"\n",
    "        \"Output only the diarized transcript.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Add speaker labels to the following transcript using logic to determine which sentence belongs to which speaker. \"\n",
    "        \"For assigning sentences: Make out the role of the speaker (for example doctor). Keep this in mind when assigning speaker names. \"\n",
    "        \"Use '[Sprecher 1]', '[Sprecher 2]', ... for the different speakers. \"\n",
    "        \"Check thoroughly for every sentence if the assignment to the speaker is contextually and logically plausible. \"\n",
    "        \"For example, in a doctor-patient-dialogue, do not assign medical questions to the Speaker who was identified as a patient before. \"\n",
    "        \"Produce a readable dialogue format between Speakers.\\n\\n\"\n",
    "        f\"{transcript}\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    return chat_vllm(messages, temperature=0.0, max_tokens=2000)\n",
    "\n",
    "if TEST_AUDIO:\n",
    "    transcript, segs, info = transcribe_faster_whisper(TEST_AUDIO, language=\"de\")\n",
    "    diarized = diarize_with_llm(transcript)\n",
    "    print(diarized)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
