{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b951ed03",
   "metadata": {},
   "source": [
    "# Vergleich: LLM-basierte vs. Pyannote-basierte Sprecherdiarisierung\n",
    "\n",
    "Dieses Notebook führt zwei Diarisierungs-Pipelines auf denselben Audios aus und speichert pro Datei:\n",
    "- Transkript (faster-whisper)\n",
    "- LLM-Diarisierung (textbasiert)\n",
    "- Pyannote-Diarisierung (audio-basiert) + Mapping auf Whisper-Segmente (nur für menschenlesbare Ausgabe)\n",
    "- Meta-Informationen inkl. Laufzeiten (Transkription / LLM / Pyannote / Post-Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports + Konfiguration\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# .env-Variablen (nicht öffentlich) laden + Fehlerbehandlung\n",
    "VLLM_BASE_URL = os.getenv(\"VLLM_BASE_URL\")\n",
    "assert VLLM_BASE_URL, \"VLLM_BASE_URL ist nicht gesetzt in .env\"\n",
    "\n",
    "VLLM_MODEL = os.getenv(\"VLLM_MODEL\")\n",
    "assert VLLM_MODEL, \"VLLM_MODEL ist nicht gesetzt in .env\"\n",
    "\n",
    "repo_root_env = os.getenv(\"REPO_ROOT\")\n",
    "assert repo_root_env, \"REPO_ROOT ist nicht gesetzt in .env\"\n",
    "REPO_ROOT = Path(repo_root_env).expanduser().resolve()\n",
    "assert REPO_ROOT.exists(), f\"REPO_ROOT existiert nicht: {REPO_ROOT}\"\n",
    "\n",
    "test_audio_env = os.getenv(\"TEST_AUDIO\")\n",
    "assert test_audio_env, \"TEST_AUDIO ist nicht gesetzt in .env\"\n",
    "TEST_AUDIO = (REPO_ROOT / test_audio_env).resolve()\n",
    "assert TEST_AUDIO.exists(), f\"TEST_AUDIO existiert nicht: {TEST_AUDIO}\"\n",
    "\n",
    "# Pfad zu gespeicherten Audios\n",
    "AUDIO_DIR = REPO_ROOT / \"data\" / \"input_audio\"\n",
    "CONVERTED_DIR = REPO_ROOT / \"data\" / \"normalised_audio\"\n",
    "\n",
    "# Ordner für Ergebnisse\n",
    "RESULTS_DIR = REPO_ROOT / \"results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61141c96",
   "metadata": {},
   "source": [
    "## Konfiguration (.env)\n",
    "\n",
    "Erwartete Variablen:\n",
    "\n",
    "- `REPO_ROOT` – Pfad zum Repo (z.B. `~/jupyter/diarization-benchmark`)\n",
    "- `TEST_AUDIO` – relativer Pfad zu einer Testdatei innerhalb des Repos\n",
    "- `VLLM_BASE_URL` – vLLM OpenAI-API Endpoint (typisch inkl. `/v1`, z.B. `http://127.0.0.1:8003/v1`)\n",
    "- `VLLM_MODEL` – served model name, z.B. google/medgemma-27b-it\n",
    "- `HUGGINGFACE_TOKEN` – Token für pyannote Modelle\n",
    "- `PYANNOTE_MODEL_ID` – z.B. `pyannote/speaker-diarization-community-1`\n",
    "\n",
    "Ordner:\n",
    "- Input: `data/input_audio/`\n",
    "- Normalisierte WAVs: `data/normalised_audio/`\n",
    "- Outputs: `results/<datei>/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775cbbff",
   "metadata": {},
   "source": [
    "## Audio-Normalisierung (ffmpeg)\n",
    "\n",
    "Für Pyannote verwenden wir einheitliche Audios: **WAV, 16 kHz, mono**.  \n",
    "Die Normalisierung wird gecached (existierende konvertierte Dateien werden wiederverwendet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Audio-Normalisierung via ffmpeg, alles konvertieren zu WAV 16khz Mono -> Wichtig für Pyannote\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_wav_16k_mono(input_path: str | Path, out_dir: str | Path) -> Path:\n",
    "    input_path = Path(input_path)\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_path = out_dir / f\"{input_path.stem}_16k_mono.wav\"\n",
    "\n",
    "    # Reuse, wenn schon vorhanden\n",
    "    if out_path.exists() and out_path.stat().st_size > 0:\n",
    "        return out_path\n",
    "\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-y\",\n",
    "        \"-i\", str(input_path),\n",
    "        \"-ac\", \"1\",        # mono\n",
    "        \"-ar\", \"16000\",    # 16kHz\n",
    "        \"-vn\",             # no video\n",
    "        str(out_path),\n",
    "    ]\n",
    "    res = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if res.returncode != 0:\n",
    "        raise RuntimeError(f\"ffmpeg failed:\\n{res.stderr}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Konvertierungs-Test\n",
    "# Testaudio mit normalisierter Version überschreiben\n",
    "conversion_test = False\n",
    "if conversion_test:\n",
    "    TEST_AUDIO = ensure_wav_16k_mono(TEST_AUDIO, CONVERTED_DIR)\n",
    "\n",
    "    print(\"WAV:\", TEST_AUDIO)\n",
    "    print(\"Exists:\", TEST_AUDIO.exists(), \"Size:\", TEST_AUDIO.stat().st_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e648463",
   "metadata": {},
   "source": [
    "## Transkribierung mit faster-whisper (lokal)\n",
    "\n",
    "Wir transkribieren jedes Audio genau einmal (auf der normalisierten WAV), um:\n",
    "- ein gemeinsames Transkript für LLM und Vergleichsausgaben zu haben\n",
    "- Whisper-Segmente als Zeitbasis für das Mapping der Pyannote-Turns zu nutzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68c6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Whisper-Config\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "WHISPER_MODEL_SIZE = os.getenv(\"WHISPER_MODEL_SIZE\", \"small\") # Kleines Modell für Tests\n",
    "# WHISPER_MODEL_SIZE = os.getenv(\"WHISPER_MODEL_SIZE\", \"large-v3\") # Großes Modell für Prod?\n",
    "WHISPER_DEVICE = os.getenv(\"WHISPER_DEVICE\", \"cpu\")  # Für unseren Test auf CPU laufen lassen -> langsamer\n",
    "# WHISPER_DEVICE = os.getenv(\"WHISPER_DEVICE\", \"cuda\") # Wenn verfügbar: Auf GPU laufen lassen -> schneller -> Aber: Konfig-Anpassungen notwendig!\n",
    "WHISPER_COMPUTE_TYPE = os.getenv(\"WHISPER_COMPUTE_TYPE\", \"int8\")  # cpu: int8 gut\n",
    "\n",
    "whisper_model = WhisperModel(WHISPER_MODEL_SIZE, device=WHISPER_DEVICE, compute_type=WHISPER_COMPUTE_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d791819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transkription\n",
    "def transcribe_faster_whisper(audio_path: str, language: str | None = None):\n",
    "    segments_iter, info = whisper_model.transcribe(\n",
    "        audio_path,\n",
    "        language=language,\n",
    "        vad_filter=True,\n",
    "        word_timestamps=False,  # später ggf. True, falls wir word-level brauchen\n",
    "    )\n",
    "    segments = []\n",
    "    texts = []\n",
    "    for seg in segments_iter:\n",
    "        txt = seg.text.strip()\n",
    "        segments.append({\"start\": float(seg.start), \"end\": float(seg.end), \"text\": txt})\n",
    "        texts.append(txt)\n",
    "    transcript = \"\\n\".join(texts).strip()\n",
    "    return transcript, segments, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ba6df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Whisper-Test anhand der Test-Audio. Aktivieren -> True setzen\n",
    "whisper_test = False\n",
    "if whisper_test:\n",
    "    t, segs, info = transcribe_faster_whisper(TEST_AUDIO, language=\"de\")\n",
    "    print(\"Language:\", info.language, \"Prob:\", info.language_probability)\n",
    "    print(\"Transcript:\\n\", t[:1000])\n",
    "    print(\"First segments:\", segs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704cd03",
   "metadata": {},
   "source": [
    "## Pipeline A: Whisper → LLM-Diarisierung (textbasiert)\n",
    "\n",
    "Eingabe: reines Transkript  \n",
    "Ausgabe: `[Sprecher 1]: ... [Sprecher 2]: ...`\n",
    "\n",
    "Diese Pipeline nutzt ausschließlich Text (keine Audioinformation). Prompts sind in `diarize_with_llm` definiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "### vLLM-Client\n",
    "\n",
    "def chat_vllm(messages, model=VLLM_MODEL, temperature=0.0, max_tokens=200, timeout=600):\n",
    "    url = f\"{VLLM_BASE_URL}/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    r = requests.post(url, json=payload, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    content = data[\"choices\"][0][\"message\"].get(\"content\", None)\n",
    "    if content is None:\n",
    "        raise RuntimeError(f\"LLM returned no content. Full response: {json.dumps(data)[:2000]}\")\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kurzer LLM-Test: Bei Bedarf auf True setzen\n",
    "llm_test = False\n",
    "\n",
    "if llm_test:\n",
    "    print(chat_vllm([{\"role\":\"user\",\"content\":\"Antworte nur mit OK\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f206fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Diarisierungs-Logik inklusive Prompts\n",
    "def diarize_with_llm(transcript: str):\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a professional conversation diarization engine. \"\n",
    "        \"Assign speaker labels logically solely based on the text. \"\n",
    "        \"Output only the diarized transcript.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Add speaker labels to the following transcript. \"\n",
    "        \"Use '[Sprecher 1]', '[Sprecher 2]', ... for the different speakers. \"\n",
    "        \"Check thoroughly for every sentence if the assignment to the speaker is contextually and logically plausible. \"\n",
    "        \"Produce a readable dialogue format between Speakers.\\n\\n\"\n",
    "        f\"{transcript}\"\n",
    "    )\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    return chat_vllm(messages, temperature=0.0, max_tokens=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fe6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kombinierter Test: Whisper-Diarisierung -> LLM-Diarisierung anhand der Test-Audio, True setzen für Test\n",
    "whisper_llm_test = False\n",
    "if whisper_llm_test:\n",
    "    print(\"Starte Transkription mit faster-whisper\")\n",
    "    t0 = time.time()\n",
    "    transcript, segs, info = transcribe_faster_whisper(TEST_AUDIO, language=\"de\")\n",
    "    t1 = time.time()\n",
    "    print(f\"Whisper-Transkription abgeschlossen in ({t1 - t0:.2f}s)\")\n",
    "    print(\"LLM-Diarisierung gestartet\")\n",
    "    t2 = time.time()\n",
    "    diarized = diarize_with_llm(transcript)\n",
    "    t3 = time.time()\n",
    "    print(f\"LLM-Diarisierung abgeschlossen in ({t3 - t2:.2f}s)\")\n",
    "    print(diarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a9219",
   "metadata": {},
   "source": [
    "## Pipeline B: Pyannote-Diarisierung (audio-basiert)\n",
    "\n",
    "Pyannote liefert alleinig **Speaker-Turns** (wer spricht wann).  \n",
    "Damit das in eine menschenlesbare Gesamt-Ausgabe überführt wird, benötigen wir **Post-Processing**:\n",
    "- Speaker-Turns auf Whisper-Segmente mappen (Overlap-Heuristik) als Kern-Prozess\n",
    "- Weitere nicht-rechenintensive kosmetische Nachbearbeitungen und Formatierungen für Vergleichbarkeit mit LLM-Ausgabe\n",
    "\n",
    "**Limitation:** Segment-basiertes Mapping kann kurze Einwürfe in langen Segmenten “verschlucken” und hat Schwierigkeiten mit ähnlichen Stimmen der Sprecher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pyannote-Setup (CPU-only)\n",
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "assert HF_TOKEN, \"HUGGINGFACE_TOKEN fehlt in .env\" # Fehlerbehandlung\n",
    "\n",
    "PYANNOTE_MODEL_ID = os.getenv(\"PYANNOTE_MODEL_ID\")\n",
    "assert PYANNOTE_MODEL_ID, \"PYANNOTE_MODEL_ID fehlt in .env\"\n",
    "\n",
    "pyannote_pipeline = Pipeline.from_pretrained(PYANNOTE_MODEL_ID, token=HF_TOKEN)\n",
    "\n",
    "# CPU erzwingen\n",
    "pyannote_pipeline.to(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pyannote starten\n",
    "def run_pyannote(wav_path: Path):\n",
    "    \n",
    "    diarization = pyannote_pipeline(str(wav_path))\n",
    "\n",
    "    return diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af7b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Minimales Pyannote Post-Processing (Mapping auf die Whisper-Segmente)\n",
    "import re\n",
    "\n",
    "### Pipeline: ensure_wav_16k_mono -> transcribe_faster_whisper -> run_pyannote -> label_whisper_segments -> normalize_speakers \n",
    "### (für Lesbarkeit, optional) -> merge_same_speakers (für Lesbarkeit, optional) -> format_dialogue (optional?)\n",
    "\n",
    "def pyannote_turns(diarization):\n",
    "    # diarization kann Annotation sein oder ein Objekt mit .speaker_diarization (Letzteres in unserem Fall)\n",
    "    ann = getattr(diarization, \"speaker_diarization\", diarization)\n",
    "    turns = []\n",
    "    for turn, _, speaker in ann.itertracks(yield_label=True):\n",
    "        turns.append((float(turn.start), float(turn.end), str(speaker)))\n",
    "    turns.sort(key=lambda x: (x[0], x[1]))\n",
    "    return turns\n",
    "\n",
    "def overlap(a0, a1, b0, b1):\n",
    "    return max(0.0, min(a1, b1) - max(a0, b0))\n",
    "\n",
    "# Haupt-Funktion zum Mappen\n",
    "def label_whisper_segments(segments, diarization, min_overlap_s=0.05):\n",
    "\n",
    "    turns = pyannote_turns(diarization)\n",
    "    labeled = []\n",
    "\n",
    "    for seg in segments:\n",
    "        s0, s1 = float(seg[\"start\"]), float(seg[\"end\"])\n",
    "        best_spk, best_ol = \"UNK\", 0.0\n",
    "\n",
    "        for t0, t1, spk in turns:\n",
    "            if t1 <= s0:\n",
    "                continue\n",
    "            if t0 >= s1:\n",
    "                break\n",
    "            ol = overlap(s0, s1, t0, t1)\n",
    "            if ol > best_ol:\n",
    "                best_ol, best_spk = ol, spk\n",
    "\n",
    "        if best_ol < min_overlap_s:\n",
    "            best_spk = \"UNK\"\n",
    "\n",
    "        labeled.append({\n",
    "            \"start\": s0, \"end\": s1,              # intern behalten\n",
    "            \"speaker_raw\": best_spk,\n",
    "            \"text\": (seg.get(\"text\") or \"\").strip()\n",
    "        })\n",
    "\n",
    "    return labeled\n",
    "\n",
    "### Kosmetische Funktion zur Vergleichbarkeit mit LLM-Ausgabe SPEAKER_00 -> Sprecher 1\n",
    "def normalize_speakers(labeled):\n",
    "    mapping = {}\n",
    "    n = 1\n",
    "    for x in labeled:\n",
    "        raw = x[\"speaker_raw\"]\n",
    "        if raw != \"UNK\" and raw not in mapping:\n",
    "            mapping[raw] = f\"[Sprecher {n}]\"\n",
    "            n += 1\n",
    "    for x in labeled:\n",
    "        x[\"speaker\"] = mapping.get(x[\"speaker_raw\"], \"UNK\")\n",
    "    return labeled\n",
    "\n",
    "def merge_same_speaker(labeled, max_gap_s=0.8):\n",
    "    # nur Lesbarkeit: aufeinanderfolgende Segmente des gleichen Speakers zusammenziehen\n",
    "    # (Gar kein Heuristik-Merging: max_gap_s=0 setzen)\n",
    "    out = []\n",
    "    for seg in labeled:\n",
    "        if not seg[\"text\"]:\n",
    "            continue\n",
    "        if not out:\n",
    "            out.append(dict(seg))\n",
    "            continue\n",
    "\n",
    "        prev = out[-1]\n",
    "        gap = seg[\"start\"] - prev[\"end\"]\n",
    "        if seg[\"speaker\"] == prev[\"speaker\"] and gap <= max_gap_s:\n",
    "            prev[\"text\"] = (prev[\"text\"].rstrip() + \" \" + seg[\"text\"].lstrip()).strip()\n",
    "            prev[\"end\"] = seg[\"end\"]\n",
    "        else:\n",
    "            out.append(dict(seg))\n",
    "    return out\n",
    "\n",
    "def format_dialogue(utterances):\n",
    "    lines = []\n",
    "    for u in utterances:\n",
    "        txt = re.sub(r\"\\s+\", \" \", u[\"text\"]).strip()\n",
    "        if txt:\n",
    "            lines.append(f\"{u['speaker']}: {txt}\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e8db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pyannote-Test mit vorheriger Transkribierung (für Segmente), bei Bedarf auf True setzen\n",
    "pyannote_test = False\n",
    "if pyannote_test:\n",
    "    print(\"Normalisiere Datei für Pyannote (wav-Datei mit 16k mono)\")\n",
    "    wav_path = ensure_wav_16k_mono(TEST_AUDIO, CONVERTED_DIR)\n",
    "    print(\"Normalisierung abgeschlossen\")\n",
    "    print(\"Starte Transkription mit faster-whisper\")\n",
    "    t0 = time.time()\n",
    "    transcript, segs, info = transcribe_faster_whisper(wav_path, language=\"de\")\n",
    "    t1 = time.time()    \n",
    "    print(f\"Whisper-Transkription abgeschlossen in ({t1 - t0:.2f}s)\")\n",
    "    print(\"Pyannote gestartet...\")\n",
    "    t2 = time.time()\n",
    "    diarization = run_pyannote(wav_path)\n",
    "    t3 = time.time()\n",
    "    print(f\"Pyannote abgeschlossen in ({t3 - t2:.2f}s)\")\n",
    "    print(\"Text-Aufbereitung begonnen\")\n",
    "    t4 = time.time()\n",
    "    # alle Schritte als eigene Variable speichern für Debugging\n",
    "    labeled = label_whisper_segments(segs, diarization, min_overlap_s=0.05) # 0.05 verbreiteter Default-Wert\n",
    "    labeled2 = normalize_speakers(labeled)\n",
    "    labeled3 = merge_same_speaker(labeled2, max_gap_s=0.8)\n",
    "    labeled4 = format_dialogue(labeled3)\n",
    "    t5 = time.time()\n",
    "    print(f\"Nachbearbeitung abgeschlossen in ({t5-t4:.2f}s)\")\n",
    "    print(f\"Gesamt-Pipeline (Pyannote + Nachbereitung) abgeschlossen in ({t5-t2:.2f}s)\")\n",
    "\n",
    "    print(\"End-Ausgabe:\\n\" + labeled4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1774b9eb",
   "metadata": {},
   "source": [
    "## Batch-Run (alle Audios in `input_audio`)\n",
    "\n",
    "Führt den Vergleich (Pyannote/LLM) für mehrere Dateien auf einmal aus und speichert Metadaten zur Auswertung.\n",
    "\n",
    "Für jede Datei wird ein eigener Ordner unter `results/` erzeugt mit:\n",
    "- `transcript.txt` als von Whisper erzeugtes Transkript zur Audio\n",
    "- `llm_diarized.txt` als der mittels LLM diarisierte Text\n",
    "- `pyannote_diarized.txt` als der mittels pyannote diarisierte Text\n",
    "- `meta.json` (Laufzeiten, Modelle, Status)\n",
    "- `whisper_segments.json` und `whisper_info.json` mit Infos zur Transkription und den Segmenten, hauptsächlich zu Debug-Zwecken\n",
    "\n",
    "Zusätzlich wird `results/batch_summary.json` als globale Datei, die sämtliche Meta-Daten enthält, erstellt (wird bei Re-Run überschrieben)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b159cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input-Audios vorbereiten und Hilfsfunktionen für das Speichern von Ergebnissen\n",
    "\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BATCH_AUDIO_DIR = REPO_ROOT / \"data\" / \"input_audio\"\n",
    "\n",
    "def save_text(path: Path, content: str):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(content or \"\", encoding=\"utf-8\")\n",
    "\n",
    "def save_json(path: Path, obj: dict):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "def list_audio_files(folder: Path) -> list[Path]:\n",
    "    exts = {\".wav\", \".mp3\", \".m4a\", \".flac\", \".ogg\"}\n",
    "    files = [p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n",
    "    return sorted(files)\n",
    "\n",
    "audio_files = list_audio_files(BATCH_AUDIO_DIR)\n",
    "print(\"Found files:\", len(audio_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Haupt-Pipeline\n",
    "### pro Datei: wav-Konvertierung -> fasterwhisper -> LLM-Diarisierung / Pyannote + Post-Processing\n",
    "### speichert sämtliche Laufzeiten und Metainformationen\n",
    "def run_compare_on_file(audio_path: Path) -> dict:\n",
    "    \n",
    "    out_dir = RESULTS_DIR / audio_path.stem\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    t0_total = time.time()\n",
    "\n",
    "    # 1) Audio normalisieren (einmal, WAV 16k Mono)\n",
    "    t0_norm = time.time()\n",
    "    wav_path = ensure_wav_16k_mono(audio_path, CONVERTED_DIR)\n",
    "    t1_norm = time.time()\n",
    "\n",
    "    # 2) Audio transkribieren (einmal, auf derselben WAV wie Pyannote)\n",
    "    t0_transc = time.time()\n",
    "    transcript, segs, info = transcribe_faster_whisper(str(wav_path), language=\"de\")\n",
    "    t1_transc = time.time()\n",
    "\n",
    "    save_text(out_dir / \"transcript.txt\", transcript)\n",
    "    save_json(out_dir / \"whisper_segments.json\", segs)  # zu Debug-Zwecken\n",
    "    save_json(out_dir / \"whisper_info.json\", {\n",
    "        \"language\": getattr(info, \"language\", None),\n",
    "        \"language_probability\": float(getattr(info, \"language_probability\", 0.0) or 0.0),\n",
    "        \"segments_count\": len(segs),\n",
    "        \"transcript_chars\": len(transcript),\n",
    "        \"transcript_words\": len(transcript.split()),\n",
    "    })\n",
    "\n",
    "    # 3) LLM-Diarisierung\n",
    "    llm_ok, llm_err = True, None\n",
    "    diarized_llm = \"\"\n",
    "    t0_llm = time.time()\n",
    "    try:\n",
    "        llm_input = transcript\n",
    "        diarized_llm = diarize_with_llm(llm_input)\n",
    "    except Exception as e:\n",
    "        llm_ok, llm_err = False, repr(e)\n",
    "    t1_llm = time.time()\n",
    "\n",
    "    save_text(out_dir / \"llm_diarized.txt\", diarized_llm)\n",
    "\n",
    "     # 4) Pyannote-Diarisierung + Post-Processing (segment-basiert)\n",
    "    py_ok, py_err = True, None\n",
    "    diarized_py = \"\"\n",
    "\n",
    "    t0_py = time.time()\n",
    "    try:\n",
    "        diarization = run_pyannote(wav_path)\n",
    "    except Exception as e:\n",
    "        py_ok, py_err = False, repr(e)\n",
    "        diarization = None\n",
    "    t1_py = time.time()\n",
    "\n",
    "    t0_post = time.time()\n",
    "    try:\n",
    "        if diarization is not None:\n",
    "            labeled = label_whisper_segments(segs, diarization, min_overlap_s=0.05)\n",
    "            labeled = normalize_speakers(labeled)\n",
    "            labeled = merge_same_speaker(labeled, max_gap_s=0.8) \n",
    "            diarized_py = format_dialogue(labeled)\n",
    "    except Exception as e:\n",
    "        py_ok, py_err = False, (py_err or \"\") + \" | postproc: \" + repr(e)\n",
    "    t1_post = time.time()\n",
    "\n",
    "    save_text(out_dir / \"pyannote_diarized.txt\", diarized_py)\n",
    "\n",
    "    t1_total = time.time()\n",
    "\n",
    "    # Meta-Informationen speichern, Zeiten auf 2 Nachkommastellen runden\n",
    "    meta = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"audio_path\": str(audio_path),\n",
    "        \"wav_path\": str(wav_path),\n",
    "        \"models\": {\n",
    "            \"whisper_model_size\": WHISPER_MODEL_SIZE,\n",
    "            \"whisper_device\": WHISPER_DEVICE,\n",
    "            \"whisper_compute_type\": WHISPER_COMPUTE_TYPE,\n",
    "            \"llm_model\": VLLM_MODEL,\n",
    "            \"pyannote_model_id\": PYANNOTE_MODEL_ID,\n",
    "        },\n",
    "        \"status\": {\n",
    "            \"llm_ok\": llm_ok,\n",
    "            \"llm_error\": llm_err,\n",
    "            \"pyannote_ok\": py_ok,\n",
    "            \"pyannote_error\": py_err,\n",
    "        },\n",
    "        \"counts\": {\n",
    "            \"whisper_segments\": len(segs),\n",
    "            \"transcript_chars\": len(transcript),\n",
    "            \"transcript_words\": len(transcript.split()),\n",
    "        },\n",
    "        \"seconds\": {\n",
    "            \"total\": round(t1_total - t0_total, 2),\n",
    "            \"normalize\": round(t1_norm - t0_norm, 2),\n",
    "            \"transcribe\": round(t1_transc - t0_transc, 2),\n",
    "            \"llm\": round(t1_llm - t0_llm, 2),\n",
    "            \"pyannote\": round(t1_py - t0_py, 2),\n",
    "            \"postproc\": round(t1_post - t0_post, 2),\n",
    "        },\n",
    "        \"params\": {\n",
    "            \"min_overlap_s\": 0.05,\n",
    "            \"merge_max_gap_s\": 0.8,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    save_json(out_dir / \"meta.json\", meta)\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0811d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ausführen der Pipeline für alle Audios und Anlegen einer globalen Summary-Datei\n",
    "\n",
    "metas = []\n",
    "failures = []\n",
    "\n",
    "for i, f in enumerate(audio_files, 1):\n",
    "    print(f\"\\n[{i}/{len(audio_files)}] {f.name}\")\n",
    "    try:\n",
    "        meta = run_compare_on_file(f)\n",
    "        metas.append(meta)\n",
    "\n",
    "        # Kontroll-Ausgaben\n",
    "        print(\n",
    "            \"  total:\", meta[\"seconds\"][\"total\"], \"s\",\n",
    "            \"| transc:\", meta[\"seconds\"][\"transcribe\"], \"s\",\n",
    "            \"| llm:\", meta[\"seconds\"][\"llm\"], \"s\",\n",
    "            \"| py:\", meta[\"seconds\"][\"pyannote\"], \"s\",\n",
    "            \"| post:\", meta[\"seconds\"][\"postproc\"], \"s\",\n",
    "            \"| ok:\", meta[\"status\"][\"llm_ok\"], \"/\", meta[\"status\"][\"pyannote_ok\"]\n",
    "        )\n",
    "    \n",
    "    # Fehlschläge direkt tracken zum Debuggen\n",
    "    except Exception as e:\n",
    "        failures.append({\"file\": str(f), \"error\": repr(e)})\n",
    "        print(\" FAIL:\", repr(e))\n",
    "\n",
    "### Alles als Gesamt-Zusammenfassung in JSON speichern für Auswertungszwecke\n",
    "save_json(RESULTS_DIR / \"batch_summary.json\", {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"files_total\": len(audio_files),\n",
    "    \"metas_count\": len(metas),\n",
    "    \"failures_count\": len(failures),\n",
    "    \"failures\": failures,\n",
    "    \"metas\": metas,\n",
    "})\n",
    "\n",
    "print(\"Done. Summary:\", RESULTS_DIR / \"batch_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
